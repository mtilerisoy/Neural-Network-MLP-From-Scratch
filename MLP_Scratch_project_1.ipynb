{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-26T12:04:21.210527Z",
     "start_time": "2023-11-26T12:04:21.150585Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def init_nn(X, Y, hidden_sizes, batch_size):\n",
    "    \"\"\"\n",
    "    Initialize the neural network structure.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X               :   Integer\n",
    "                        Input shape\n",
    "    \n",
    "    Y               :   Integer\n",
    "                        Output shape\n",
    "    \n",
    "    hidden_sizes    :   List\n",
    "                        List of length equal to number of hidden layers and each entry is the number of\n",
    "                        neurons in that layer\n",
    "    \n",
    "    batch_size      :   Integer\n",
    "                        Number of batch to be fed in one forward pass\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    layer_states    :   List of Arrays\n",
    "                        A list of arrays containing outputs of each layer. Initialized as an empty array\n",
    "                        to increase the speed of initialization\n",
    "    \n",
    "    weights         :   List of Arrays\n",
    "                        A list of arrays containing the weights of each layer. Initialized randomly\n",
    "                        between -1 and 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Concatenate sizes of each layer including input and output into a list, generates list of arrays\n",
    "    layer_sizes = np.concatenate(([X], hidden_sizes, [Y]))\n",
    "    \n",
    "    # Create an empty list of arrays to store the neuron outputs (i.e. states) for each layer\n",
    "    layer_states = [np.empty((batch_size,layer_size)) for layer_size in layer_sizes]\n",
    "    \n",
    "    # Initialize the weights of the network given the sizes of the layers\n",
    "    weights = list()\n",
    "    for i in range(layer_sizes.shape[0]-1):\n",
    "        \n",
    "        # Randomly initialize the weights for each layer between -1 and 1\n",
    "        weights.append(np.random.uniform(-1,1,size=[layer_sizes[i],layer_sizes[i+1]]))\n",
    "    \n",
    "    return layer_states, weights\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T11:47:13.733710Z",
     "start_time": "2023-11-26T11:47:13.710416Z"
    }
   },
   "id": "328c4befb5397e51"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of input `x` element-wise\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x           :   Float or Array\n",
    "                    input\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sigmoid(x)  :   Float or Array\n",
    "                    sigmoid applied to input `x` element-wise\n",
    "    \"\"\"\n",
    "    \n",
    "    # Return the sigmoid formula, computed element-wise\n",
    "    return 1./(1.+np.exp((-1)*x))\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Computes the softmax of input `x`,\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x           :   Array\n",
    "                    (N x dim) array with N samples by p dimensions. dim=10 for MNIST classification. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    softmax(x)  :   float or array_like\n",
    "                    softmax applied to `x` along the first axis.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the exponent only once\n",
    "    exponent = np.exp(x)\n",
    "    \n",
    "    # Return the softmax function\n",
    "    return exponent/exponent.sum(axis=1,keepdims=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T11:47:13.765125Z",
     "start_time": "2023-11-26T11:47:13.730915Z"
    }
   },
   "id": "1e0f60dbd82ae077"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def feedforward(batch, layer_states, weights):\n",
    "    \"\"\"\n",
    "    Takes the input batch and feeds it through the generated network ONCE.\n",
    "    Returns the output of last layer fed through softmax function and updated layer outputs\n",
    "    \n",
    "    Possible Modification:\n",
    "    feeding the last output to softmax can be done outside the function and\n",
    "    this func can return only updated layer outputs\n",
    "    OOOR\n",
    "    maybe there is no need to return the updated version of neuron values\n",
    "    return only the output (without softmax)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    batch           :   Array\n",
    "                        Input batch of dimension [batch_size x Input_dimension] \n",
    "    \n",
    "    layer_states    :   List of Arrays\n",
    "                        Output arrays of each layer\n",
    "    \n",
    "    weights         :   List of Arrays\n",
    "                        Weights array of each layer \n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    output          :   Array\n",
    "                        Output of the last layer (output layer)\n",
    "    \n",
    "    layer_states    :   List of Arrays\n",
    "                        UPDATED output arrays of each layer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the state of input layer to the input batch\n",
    "    #layer_states[0] = batch\n",
    "    \n",
    "    # For testing randomly initialize the input layer\n",
    "    h_l = np.random.uniform(-1,1,size=[batch_size,X])\n",
    "    layer_states[0] = h_l\n",
    "    \n",
    "    for i,weight in enumerate(weights):\n",
    "        #h_l = sigmoid(h_l.dot(weight))\n",
    "        #hidden_layers[i+1]=h_l\n",
    "        \n",
    "        # Calculate the dot product of current layer's neuron values and weights and feed it to sigmoid\n",
    "        layer_states[i+1] = sigmoid(layer_states[i].dot(weight))\n",
    "    \n",
    "    # Feed the output layers neuron values into softmax function \n",
    "    output = softmax(layer_states[-1])\n",
    "    \n",
    "    # Return the outputs and updated version of neuron values\n",
    "    return output, layer_states\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T11:47:13.765386Z",
     "start_time": "2023-11-26T11:47:13.736905Z"
    }
   },
   "id": "1ca0f7657d2fc382"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 10\u001B[0m\n\u001B[1;32m      6\u001B[0m layer_states, weights \u001B[38;5;241m=\u001B[39m init_nn(X, Y, hidden_sizes, batch_size)\n\u001B[1;32m      8\u001B[0m output, layer_states \u001B[38;5;241m=\u001B[39m feedforward(\u001B[38;5;28;01mNone\u001B[39;00m, layer_states, weights)\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;43mprint\u001B[39;49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124masd\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[5], line 10\u001B[0m\n\u001B[1;32m      6\u001B[0m layer_states, weights \u001B[38;5;241m=\u001B[39m init_nn(X, Y, hidden_sizes, batch_size)\n\u001B[1;32m      8\u001B[0m output, layer_states \u001B[38;5;241m=\u001B[39m feedforward(\u001B[38;5;28;01mNone\u001B[39;00m, layer_states, weights)\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;43mprint\u001B[39;49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124masd\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_38_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_38_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_38_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_38_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_38_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_38_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_38_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_38_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_38_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_38_64.PyDBFrame.do_wait_suspend\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the input and output shape\n",
    "X = 784\n",
    "Y = 10\n",
    "\n",
    "# Initialize the batch size and hidden layers shape\n",
    "batch_size = 8\n",
    "hidden_sizes = [4,2,3]\n",
    "\n",
    "# Initialize the network structure\n",
    "layer_states, weights = init_nn(X, Y, hidden_sizes, batch_size)\n",
    "\n",
    "# Feed forward the input - once\n",
    "output, layer_states = feedforward(None, layer_states, weights)\n",
    "\n",
    "print(\"asd\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T11:47:23.388517Z",
     "start_time": "2023-11-26T11:47:15.480852Z"
    }
   },
   "id": "373396cca0847fc4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def d_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate derivative of sigmoid activation based on sigmoid output.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigmoid_out : array_like\n",
    "        Output values processed by a sigmoid function.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sigmoid_prime(h) : array_like\n",
    "        Derivative of sigmoid, based on value of sigmoid.\n",
    "    \"\"\"\n",
    "    return x*(1-x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27b70022e2b7f2d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def back_prop(output,batch_y,hidden_layers,weights,batch_size,lr):\n",
    "        \"\"\"\n",
    "        Calculate derivative of sigmoid activation based on sigmoid output.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output : array_like\n",
    "            Forward pass output of the MLP\n",
    "        batch_y : array_like\n",
    "            True labels for the samples in the batch\n",
    "        hidden_layers : list\n",
    "            List of hidden layer outputs  \n",
    "        weights : array_like\n",
    "            Array of weight matricies\n",
    "        lr : float\n",
    "            Learning rate for SGD\n",
    "        batch_size : int\n",
    "            Size of a training mini-batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weights : array_like\n",
    "            Array of weight matricies, updated from the backpropagation.\n",
    "    \n",
    "        \"\"\"\n",
    "        delta_t = (output - batch_y)*sigmoid_prime(hidden_layers[-1])\n",
    "        for i in range(1,len(weights)+1):\n",
    "            weights[-i]-=lr*(hidden_layers[-i-1].T.dot(delta_t))/batch_size\n",
    "            delta_t = sigmoid_prime(hidden_layers[-i-1])*(delta_t.dot(weights[-i].T))\n",
    "        return weights"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78bab426f3140caa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def __back_prop(self,batch_y):\n",
    "        # Update the weights of the network through back-propagation\n",
    "        delta_t = (self.__out - batch_y)*self.__sigmoid_prime(self.__h[-1])\n",
    "        for i in range(1,len(self.weights)+1):\n",
    "            self.weights[-i]-=self.lr*(self.__h[-i-1].T.dot(delta_t))/self.batch_size\n",
    "            delta_t = self.__sigmoid_prime(self.__h[-i-1])*(delta_t.dot(self.weights[-i].T))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a405633b5e617828"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
